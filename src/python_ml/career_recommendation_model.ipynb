
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Recommendation System - KNN Model Development\n",
    "\n",
    "This notebook trains and evaluates a K-Nearest Neighbors (KNN) model for career recommendations based on student data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas scikit-learn matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Your Own Dataset\n",
    "\n",
    "This section allows you to upload your own CSV file containing student profile data and career recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own CSV dataset\n",
    "print(\"Please upload your CSV file containing student data and career recommendations.\")\n",
    "print(\"Your CSV should contain columns for student attributes (interests, skills, academic scores) and target career recommendations.\")\n",
    "print(\"\\nWaiting for file upload...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check if a file was uploaded\n",
    "if uploaded:\n",
    "    # Get the first uploaded file name\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    \n",
    "    print(f\"\\nFile '{file_name}' uploaded successfully!\")\n",
    "    \n",
    "    # Try to read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f\"\\nDataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        \n",
    "        # Display the first few rows\n",
    "        print(\"\\nFirst 5 rows of the dataset:\")\n",
    "        df.head()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV file: {e}\")\n",
    "        print(\"\\nGenerating a sample dataset instead...\")\n",
    "        df = None\n",
    "else:\n",
    "    print(\"\\nNo file uploaded. Generating a sample dataset instead...\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset if no file was uploaded or loading failed\n",
    "if df is None:\n",
    "    print(\"Creating a sample dataset for demonstration purposes...\")\n",
    "    \n",
    "    # Number of samples\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Career options\n",
    "    careers = [\n",
    "        \"Software Engineer\", \"Data Scientist\", \"Graphic Designer\", \n",
    "        \"Marketing Manager\", \"Systems Analyst\", \"Content Writer\",\n",
    "        \"Doctor\", \"Lawyer\", \"Financial Analyst\", \"Civil Engineer\",\n",
    "        \"Mechanical Engineer\", \"Teacher\", \"Research Scientist\", \n",
    "        \"HR Manager\", \"Entrepreneur\"\n",
    "    ]\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Create feature dataframe\n",
    "    data = {\n",
    "        # Features scaled 1-5\n",
    "        'interests_technical': np.random.randint(1, 6, n_samples),\n",
    "        'interests_creative': np.random.randint(1, 6, n_samples),\n",
    "        'interests_social': np.random.randint(1, 6, n_samples),\n",
    "        'interests_investigative': np.random.randint(1, 6, n_samples),\n",
    "        'skills_analytical': np.random.randint(1, 6, n_samples),\n",
    "        'skills_communication': np.random.randint(1, 6, n_samples),\n",
    "        'skills_technical': np.random.randint(1, 6, n_samples),\n",
    "        'skills_problem_solving': np.random.randint(1, 6, n_samples),\n",
    "        'academic_science': np.random.randint(1, 6, n_samples),\n",
    "        'academic_humanities': np.random.randint(1, 6, n_samples),\n",
    "        'academic_commerce': np.random.randint(1, 6, n_samples),\n",
    "    }\n",
    "    \n",
    "    # Creating correlation between features and careers\n",
    "    # This is a simplified approach - in real model, correlations would be more complex\n",
    "    career_list = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Simplified logic to assign careers based on feature values\n",
    "        if data['interests_technical'][i] > 3 and data['skills_analytical'][i] > 3 and data['academic_science'][i] > 3:\n",
    "            # Technical roles\n",
    "            career_list.append(np.random.choice([\"Software Engineer\", \"Data Scientist\", \"Systems Analyst\", \"Mechanical Engineer\"]))\n",
    "            \n",
    "        elif data['interests_creative'][i] > 3 and data['skills_communication'][i] > 3:\n",
    "            # Creative roles\n",
    "            career_list.append(np.random.choice([\"Graphic Designer\", \"Content Writer\", \"Marketing Manager\"]))\n",
    "            \n",
    "        elif data['interests_social'][i] > 3 and data['academic_humanities'][i] > 3:\n",
    "            # Social/humanities roles\n",
    "            career_list.append(np.random.choice([\"Teacher\", \"HR Manager\", \"Lawyer\"]))\n",
    "            \n",
    "        elif data['interests_investigative'][i] > 3 and data['academic_science'][i] > 3:\n",
    "            # Research/scientific roles\n",
    "            career_list.append(np.random.choice([\"Research Scientist\", \"Doctor\", \"Civil Engineer\"]))\n",
    "            \n",
    "        elif data['skills_analytical'][i] > 3 and data['academic_commerce'][i] > 3:\n",
    "            # Business/finance roles\n",
    "            career_list.append(np.random.choice([\"Financial Analyst\", \"Entrepreneur\"]))\n",
    "            \n",
    "        else:\n",
    "            # Random assignment for cases not fitting above patterns\n",
    "            career_list.append(np.random.choice(careers))\n",
    "    \n",
    "    # Add the target variable to the dataset\n",
    "    data['recommended_career'] = career_list\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the synthetic dataset to CSV\n",
    "    df.to_csv('student_data_sample.csv', index=False)\n",
    "    print(\"Sample dataset created and saved as 'student_data_sample.csv'\")\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nFirst 5 rows of the sample dataset:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection and Validation\n",
    "\n",
    "Let's examine the structure of the dataset and ensure it's suitable for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column data types and missing values\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values if any\n",
    "if df.isnull().values.any():\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    print(\"Missing values handled.\")\n",
    "    print(\"\\nRemaining missing values:\")\n",
    "    print(df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the target variable (career recommendation column)\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Ask user to identify the target column if not already known\n",
    "target_col = None\n",
    "possible_target_cols = [col for col in df.columns if 'career' in col.lower() or 'profession' in col.lower() or 'job' in col.lower() or 'occupation' in col.lower() or 'recommended' in col.lower()]\n",
    "\n",
    "if len(possible_target_cols) == 1:\n",
    "    target_col = possible_target_cols[0]\n",
    "    print(f\"\\nAutomatically identified target column: '{target_col}'\")\n",
    "elif len(possible_target_cols) > 1:\n",
    "    print(f\"\\nFound multiple possible target columns: {possible_target_cols}\")\n",
    "    # For simplicity, selecting the first one, but in a real notebook you'd want to ask the user\n",
    "    target_col = possible_target_cols[0]\n",
    "    print(f\"Using '{target_col}' as the target column\")\n",
    "else:\n",
    "    # If no target column is found, default to the last column for the sample dataset\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\"\\nNo target column automatically detected. Using the last column '{target_col}' as the target.\")\n",
    "\n",
    "# Verify the target column looks correct\n",
    "print(f\"\\nUnique values in the target column '{target_col}':\")\n",
    "print(df[target_col].value_counts().head(10))  # Show top 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for numeric columns\n",
    "print(\"\\nBasic Statistics:\")\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of recommended careers\n",
    "plt.figure(figsize=(12, 8))\n",
    "career_counts = df[target_col].value_counts()\n",
    "\n",
    "# Limit to top 15 careers for better visualization if there are many\n",
    "if len(career_counts) > 15:\n",
    "    print(f\"Showing top 15 out of {len(career_counts)} unique career values\")\n",
    "    career_counts = career_counts.head(15)\n",
    "\n",
    "sns.barplot(x=career_counts.values, y=career_counts.index)\n",
    "plt.title(f'Distribution of {target_col}')\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature categories if they exist in the dataset\n",
    "feature_categories = {}\n",
    "\n",
    "# Look for common prefixes in column names to group them\n",
    "prefixes = set()\n",
    "for col in df.columns:\n",
    "    if col == target_col:\n",
    "        continue\n",
    "    parts = col.split('_')\n",
    "    if len(parts) > 1:\n",
    "        prefixes.add(parts[0])\n",
    "\n",
    "# Group columns by prefixes\n",
    "for prefix in prefixes:\n",
    "    feature_categories[prefix.capitalize()] = [col for col in df.columns if col.startswith(prefix + '_')]\n",
    "\n",
    "# If no categories were found, create a single 'Features' category with all non-target columns\n",
    "if not feature_categories:\n",
    "    feature_categories['Features'] = [col for col in df.columns if col != target_col]\n",
    "\n",
    "print(\"\\nIdentified feature categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category}: {len(features)} features\")\n",
    "    print(f\"  Example features: {', '.join(features[:3])}{'...' if len(features) > 3 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for features grouped by top careers\n",
    "# Select top 5 careers by frequency for clearer visualization\n",
    "top_careers = career_counts.index[:5].tolist()\n",
    "filtered_df = df[df[target_col].isin(top_careers)]\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if len(features) > 0:\n",
    "        # Only select numeric features\n",
    "        numeric_features = [f for f in features if df[f].dtype in ['int64', 'float64']]\n",
    "        \n",
    "        if numeric_features:\n",
    "            plt.figure(figsize=(15, max(10, len(numeric_features) * 2)))\n",
    "            plt.suptitle(f'Distribution of {category} by Top 5 Careers', fontsize=16)\n",
    "            \n",
    "            for i, feature in enumerate(numeric_features, 1):\n",
    "                plt.subplot(len(numeric_features), 1, i)\n",
    "                sns.boxplot(x=target_col, y=feature, data=filtered_df)\n",
    "                plt.title(feature.replace('_', ' ').title())\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "            \n",
    "            plt.subplots_adjust(top=0.95)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns that need encoding\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove(target_col) if target_col in categorical_cols else None\n",
    "\n",
    "# Encode categorical features if any\n",
    "if categorical_cols:\n",
    "    print(f\"Encoding {len(categorical_cols)} categorical features...\")\n",
    "    for col in categorical_cols:\n",
    "        # One-hot encode categorical columns\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    print(\"Categorical features encoded.\")\n",
    "else:\n",
    "    print(\"No categorical features to encode.\")\n",
    "\n",
    "# Encode the target variable\n",
    "print(f\"\\nEncoding target variable '{target_col}'...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_target'] = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "# Map encoded values back to original labels\n",
    "target_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "print(f\"\\nTarget encoding mapping (first 5):\\n{dict(list(target_mapping.items())[:5])}\")\n",
    "\n",
    "# Save the label encoder for later use\n",
    "joblib.dump(label_encoder, 'career_label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop([target_col, 'encoded_target'], axis=1)\n",
    "y = df['encoded_target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'career_scaler.pkl')\n",
    "print(\"StandardScaler saved to 'career_scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Development - K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal value of k using cross-validation\n",
    "k_values = list(range(1, 31, 2))\n",
    "cross_val_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "    cross_val_scores.append(scores.mean())\n",
    "\n",
    "# Plot k values vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cross_val_scores, 'o-')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Optimal k Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best k\n",
    "best_k = k_values[cross_val_scores.index(max(cross_val_scores))]\n",
    "print(f\"Best k value: {best_k} with accuracy: {max(cross_val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best k value\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(knn, 'career_knn_model.pkl')\n",
    "print(\"KNN model saved to 'career_knn_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Convert encoded predictions back to original career labels\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Get unique class labels\n",
    "class_names = [target_mapping[i] for i in range(len(target_mapping))]\n",
    "\n",
    "# If there are too many classes, only show top classes\n",
    "if len(class_names) > 10:\n",
    "    # Get top 10 most frequent classes in test set\n",
    "    top_classes_idx = y_test.value_counts().head(10).index\n",
    "    mask = np.isin(y_test, top_classes_idx) & np.isin(y_pred, top_classes_idx)\n",
    "    \n",
    "    # Filter to only include top classes\n",
    "    y_test_filtered = y_test[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Recompute confusion matrix with filtered data\n",
    "    cm = confusion_matrix(y_test_filtered, y_pred_filtered)\n",
    "    \n",
    "    # Get class names for the filtered matrix\n",
    "    class_names = [target_mapping[i] for i in sorted(np.unique(y_test_filtered))]\n",
    "    \n",
    "    print(f\"Showing confusion matrix for top {len(class_names)} classes only\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(min(14, len(class_names)), min(12, len(class_names))))\n",
    "\n",
    "# Generate the confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, \n",
    "            yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for KNN using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# This may take some time for larger datasets\n",
    "print(\"Calculating feature importances (this may take a few minutes)...\")\n",
    "result = permutation_importance(\n",
    "    knn, X_test_scaled, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Sort features by importance\n",
    "importances = result.importances_mean\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot top 20 features or all if less than 20\n",
    "n_features_to_plot = min(20, len(feature_names))\n",
    "top_indices = indices[:n_features_to_plot]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(f'Top {n_features_to_plot} Feature Importances')\n",
    "plt.barh(range(n_features_to_plot), importances[top_indices])\n",
    "plt.yticks(range(n_features_to_plot), [feature_names[i] for i in top_indices])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with the model\n",
    "def predict_career(input_data):\n",
    "    \"\"\"\n",
    "    Predict career based on input features.\n",
    "    \n",
    "    Parameters:\n",
    "    input_data (dict): Dictionary with feature names and values\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Top predicted career and list of (career, probability) tuples\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with the input features\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Make sure input_df has the same columns as the training data\n",
    "    for col in X.columns:\n",
    "        if col not in input_df.columns:\n",
    "            input_df[col] = 0  # Default value for missing columns\n",
    "    \n",
    "    # Ensure column order matches training data\n",
    "    input_df = input_df[X.columns]\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    \n",
    "    # Get predicted career\n",
    "    prediction_encoded = knn.predict(input_scaled)[0]\n",
    "    prediction = target_mapping[prediction_encoded]\n",
    "    \n",
    "    # Get probabilities for each class\n",
    "    # For KNN, we can use predict_proba which returns the fraction of neighbors from each class\n",
    "    probabilities = knn.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    # Get top 5 careers with their probabilities\n",
    "    career_probs = [(target_mapping[i], prob * 100) for i, prob in enumerate(probabilities)]\n",
    "    top_careers = sorted(career_probs, key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    return prediction, top_careers\n",
    "\n",
    "# Create an example input based on the features in the dataset\n",
    "example_input = {}\n",
    "for col in X.columns[:10]:  # Using first 10 features for the example\n",
    "    # For simplicity, use random values between min and max of each feature\n",
    "    min_val = X[col].min()\n",
    "    max_val = X[col].max()\n",
    "    \n",
    "    # Generate a random value in the feature's range\n",
    "    if X[col].dtype in ['int64', 'int32']:\n",
    "        example_input[col] = np.random.randint(min_val, max_val + 1)\n",
    "    else:\n",
    "        example_input[col] = np.random.uniform(min_val, max_val)\n",
    "\n",
    "# Print the example input\n",
    "print(\"Example input:\")\n",
    "for key, value in example_input.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Make a prediction\n",
    "prediction, top_careers = predict_career(example_input)\n",
    "\n",
    "print(f\"\\nTop predicted career: {prediction}\\n\")\n",
    "print(\"Top 5 career recommendations:\")\n",
    "for career, prob in top_careers:\n",
    "    print(f\"{career}: {prob:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions for our example input\n",
    "plt.figure(figsize=(10, 6))\n",
    "careers, scores = zip(*top_careers)\n",
    "colors = ['#FF9999' if i == 0 else '#99CCFF' for i in range(len(top_careers))]\n",
    "plt.bar(careers, scores, color=colors)\n",
    "plt.title('Top 5 Career Recommendations')\n",
    "plt.xlabel('Career')\n",
    "plt.ylabel('Confidence Score (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 100)\n",
    "for i, (_, score) in enumerate(zip(careers, scores)):\n",
    "    plt.text(i, score + 1, f\"{score:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download the Trained Model and Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model and necessary files for use in your application\n",
    "files.download('career_knn_model.pkl')\n",
    "files.download('career_scaler.pkl')\n",
    "files.download('career_label_encoder.pkl')\n",
    "files.download('student_data_sample.csv') if 'student_data_sample.csv' in os.listdir() else None\n",
    "\n",
    "print(\"\\nFiles ready for download:\")\n",
    "print(\"1. career_knn_model.pkl - The trained KNN model\")\n",
    "print(\"2. career_scaler.pkl - The feature scaler\")\n",
    "print(\"3. career_label_encoder.pkl - The label encoder for career names\")\n",
    "print(\"4. student_data_sample.csv - The sample dataset (if created)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Guide - How to Use the Model in Your Application\n",
    "\n",
    "To integrate this model into your career guidance application, follow these steps:\n",
    "\n",
    "### Step 1: Save the model files\n",
    "Download all the generated files (`career_knn_model.pkl`, `career_scaler.pkl`, and `career_label_encoder.pkl`) and place them in your application's directory.\n",
    "\n",
    "### Step 2: Load the model in your Python backend\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model and preprocessors\n",
    "knn_model = joblib.load('career_knn_model.pkl')\n",
    "scaler = joblib.load('career_scaler.pkl')\n",
    "label_encoder = joblib.load('career_label_encoder.pkl')\n",
    "\n",
    "def predict_career(user_data):\n",
    "    \"\"\"\n",
    "    Make career predictions based on user input data\n",
    "    \n",
    "    Parameters:\n",
    "    user_data (dict): Dictionary containing user profile data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Convert user data to DataFrame\n",
    "    input_df = pd.DataFrame([user_data])\n",
    "    \n",
    "    # Ensure all model features are present\n",
    "    model_features = joblib.load('model_features.pkl')  # You'll need to save this during training\n",
    "    for feature in model_features:\n",
    "        if feature not in input_df.columns:\n",
    "            input_df[feature] = 0\n",
    "    \n",
    "    # Keep only the features the model was trained on\n",
    "    input_df = input_df[model_features]\n",
    "    \n",
    "    # Scale the input\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    \n",
    "    # Get predictions\n",
    "    prediction_encoded = knn_model.predict(input_scaled)[0]\n",
    "    probabilities = knn_model.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    # Convert encoding back to career names\n",
    "    prediction = label_encoder.inverse_transform([prediction_encoded])[0]\n",
    "    \n",
    "    # Get top 5 careers with probabilities\n",
    "    career_indices = np.argsort(probabilities)[::-1][:5]\n",
    "    top_careers = [\n",
    "        {\n",
    "            'career': label_encoder.inverse_transform([idx])[0],\n",
    "            'confidence': float(probabilities[idx] * 100)\n",
    "        }\n",
    "        for idx in career_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'top_prediction': prediction,\n",
    "        'career_matches': top_careers\n",
    "    }\n",
    "```\n",
    "\n",
    "### Step 3: Create an API endpoint (using Flask, FastAPI, etc.)\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    user_data = request.json\n",
    "    result = predict_career(user_data)\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "### Step 4: Connect your frontend to the API\n",
    "```javascript\n",
    "// In your React application\n",
    "async function getCareerPrediction(userData) {\n",
    "  try {\n",
    "    const response = await fetch('http://your-api-url/predict', {\n",
    "      method: 'POST',\n",
    "      headers: {\n",
    "        'Content-Type': 'application/json',\n",
    "      },\n",
    "      body: JSON.stringify(userData),\n",
    "    });\n",
    "    \n",
    "    const result = await response.json();\n",
    "    return result;\n",
    "  } catch (error) {\n",
    "    console.error('Error fetching prediction:', error);\n",
    "    return null;\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 5: Regular model updates\n",
    "As you collect more user data, periodically retrain your model using this notebook with your expanded dataset to improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. **Data Loading**: Loading and preprocessing your own CSV dataset\n",
    "2. **Exploratory Data Analysis**: Visualizing data distributions and relationships\n",
    "3. **Model Training**: Finding the optimal K value and training a KNN model\n",
    "4. **Model Evaluation**: Calculating accuracy, confusion matrix, and other metrics\n",
    "5. **Feature Importance**: Identifying which features most influence career recommendations\n",
    "6. **Making Predictions**: Demonstrating how to use the model for new students\n",
    "7. **Integration Guide**: Steps to integrate the model into your application\n",
    "\n",
    "The trained model can now be integrated into your career guidance application to provide personalized recommendations to students based on their profiles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
