
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Recommendation System - KNN Model Development\n",
    "\n",
    "This notebook trains and evaluates a K-Nearest Neighbors (KNN) model for career recommendations based on student data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas scikit-learn matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "You can either upload your own student dataset CSV or use the sample dataset generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload your own dataset\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
    "    \n",
    "    # Assuming the first uploaded file is the CSV\n",
    "    if len(uploaded.keys()) > 0:\n",
    "        first_file = list(uploaded.keys())[0]\n",
    "        df = pd.read_csv(first_file)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(\"\\nDataset shape:\", df.shape)\n",
    "        df.head()\n",
    "    else:\n",
    "        print(\"No file uploaded, will create sample dataset instead.\")\n",
    "        raise FileNotFoundError\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading the uploaded file: {e}\")\n",
    "    print(\"Creating a sample dataset instead...\")\n",
    "    \n",
    "    # Option 2: Create a sample dataset\n",
    "    # Number of samples\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Career options\n",
    "    careers = [\n",
    "        \"Software Engineer\", \"Data Scientist\", \"Graphic Designer\", \n",
    "        \"Marketing Manager\", \"Systems Analyst\", \"Content Writer\",\n",
    "        \"Doctor\", \"Lawyer\", \"Financial Analyst\", \"Civil Engineer\",\n",
    "        \"Mechanical Engineer\", \"Teacher\", \"Research Scientist\", \n",
    "        \"HR Manager\", \"Entrepreneur\"\n",
    "    ]\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Create feature dataframe\n",
    "    data = {\n",
    "        # Features scaled 1-5\n",
    "        'interests_technical': np.random.randint(1, 6, n_samples),\n",
    "        'interests_creative': np.random.randint(1, 6, n_samples),\n",
    "        'interests_social': np.random.randint(1, 6, n_samples),\n",
    "        'interests_investigative': np.random.randint(1, 6, n_samples),\n",
    "        'skills_analytical': np.random.randint(1, 6, n_samples),\n",
    "        'skills_communication': np.random.randint(1, 6, n_samples),\n",
    "        'skills_technical': np.random.randint(1, 6, n_samples),\n",
    "        'skills_problem_solving': np.random.randint(1, 6, n_samples),\n",
    "        'academic_science': np.random.randint(1, 6, n_samples),\n",
    "        'academic_humanities': np.random.randint(1, 6, n_samples),\n",
    "        'academic_commerce': np.random.randint(1, 6, n_samples),\n",
    "    }\n",
    "    \n",
    "    # Creating correlation between features and careers\n",
    "    # This is a simplified approach - in real model, correlations would be more complex\n",
    "    career_list = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Simplified logic to assign careers based on feature values\n",
    "        if data['interests_technical'][i] > 3 and data['skills_analytical'][i] > 3 and data['academic_science'][i] > 3:\n",
    "            # Technical roles\n",
    "            career_list.append(np.random.choice([\"Software Engineer\", \"Data Scientist\", \"Systems Analyst\", \"Mechanical Engineer\"]))\n",
    "            \n",
    "        elif data['interests_creative'][i] > 3 and data['skills_communication'][i] > 3:\n",
    "            # Creative roles\n",
    "            career_list.append(np.random.choice([\"Graphic Designer\", \"Content Writer\", \"Marketing Manager\"]))\n",
    "            \n",
    "        elif data['interests_social'][i] > 3 and data['academic_humanities'][i] > 3:\n",
    "            # Social/humanities roles\n",
    "            career_list.append(np.random.choice([\"Teacher\", \"HR Manager\", \"Lawyer\"]))\n",
    "            \n",
    "        elif data['interests_investigative'][i] > 3 and data['academic_science'][i] > 3:\n",
    "            # Research/scientific roles\n",
    "            career_list.append(np.random.choice([\"Research Scientist\", \"Doctor\", \"Civil Engineer\"]))\n",
    "            \n",
    "        elif data['skills_analytical'][i] > 3 and data['academic_commerce'][i] > 3:\n",
    "            # Business/finance roles\n",
    "            career_list.append(np.random.choice([\"Financial Analyst\", \"Entrepreneur\"]))\n",
    "            \n",
    "        else:\n",
    "            # Random assignment for cases not fitting above patterns\n",
    "            career_list.append(np.random.choice(careers))\n",
    "    \n",
    "    # Add the target variable to the dataset\n",
    "    data['recommended_career'] = career_list\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the synthetic dataset to CSV\n",
    "    df.to_csv('student_data.csv', index=False)\n",
    "    print(\"Sample dataset created and saved as 'student_data.csv'\")\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of recommended careers\n",
    "plt.figure(figsize=(12, 8))\n",
    "career_counts = df['recommended_career'].value_counts()\n",
    "sns.barplot(x=career_counts.values, y=career_counts.index)\n",
    "plt.title('Distribution of Recommended Careers')\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for each feature grouped by career\n",
    "# This helps visualize how different careers relate to different interest/skill levels\n",
    "feature_categories = {\n",
    "    'Interests': [col for col in df.columns if col.startswith('interests_')],\n",
    "    'Skills': [col for col in df.columns if col.startswith('skills_')],\n",
    "    'Academics': [col for col in df.columns if col.startswith('academic_')]\n",
    "}\n",
    "\n",
    "# Select top 5 careers by frequency for clearer visualization\n",
    "top_careers = career_counts.index[:5].tolist()\n",
    "filtered_df = df[df['recommended_career'].isin(top_careers)]\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f'Distribution of {category} by Top 5 Careers', fontsize=16)\n",
    "    \n",
    "    for i, feature in enumerate(features, 1):\n",
    "        plt.subplot(len(features), 1, i)\n",
    "        sns.boxplot(x='recommended_career', y=feature, data=filtered_df)\n",
    "        plt.title(feature.replace('_', ' ').title())\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('recommended_career', axis=1)\n",
    "y = df['recommended_career']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'career_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development - K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal value of k using cross-validation\n",
    "k_values = list(range(1, 31, 2))\n",
    "cross_val_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "    cross_val_scores.append(scores.mean())\n",
    "\n",
    "# Plot k values vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cross_val_scores, 'o-')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Optimal k Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best k\n",
    "best_k = k_values[cross_val_scores.index(max(cross_val_scores))]\n",
    "print(f\"Best k value: {best_k} with accuracy: {max(cross_val_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best k value\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(knn, 'career_knn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a list of unique classes\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "# Generate the confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, \n",
    "            yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for KNN\n",
    "# For KNN we'll use permutation importance technique\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    knn, X_test_scaled, y_test, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance for KNN Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Making Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with the model\n",
    "def predict_career(interests_technical, interests_creative, interests_social, interests_investigative,\n",
    "                  skills_analytical, skills_communication, skills_technical, skills_problem_solving,\n",
    "                  academic_science, academic_humanities, academic_commerce):\n",
    "    \n",
    "    # Create a DataFrame with the input features\n",
    "    input_data = pd.DataFrame({\n",
    "        'interests_technical': [interests_technical],\n",
    "        'interests_creative': [interests_creative],\n",
    "        'interests_social': [interests_social],\n",
    "        'interests_investigative': [interests_investigative],\n",
    "        'skills_analytical': [skills_analytical],\n",
    "        'skills_communication': [skills_communication],\n",
    "        'skills_technical': [skills_technical],\n",
    "        'skills_problem_solving': [skills_problem_solving],\n",
    "        'academic_science': [academic_science],\n",
    "        'academic_humanities': [academic_humanities],\n",
    "        'academic_commerce': [academic_commerce]\n",
    "    })\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Get predicted career\n",
    "    prediction = knn.predict(input_scaled)[0]\n",
    "    \n",
    "    # Get probabilities for each class\n",
    "    # For KNN, we can use predict_proba which returns the fraction of neighbors from each class\n",
    "    probabilities = knn.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    # Get top 5 careers with their probabilities\n",
    "    careers = knn.classes_\n",
    "    career_probs = [(career, prob * 100) for career, prob in zip(careers, probabilities)]\n",
    "    top_careers = sorted(career_probs, key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    return prediction, top_careers\n",
    "\n",
    "# Example prediction\n",
    "# Values are on scale of 1-5\n",
    "prediction, top_careers = predict_career(\n",
    "    interests_technical=4, \n",
    "    interests_creative=3, \n",
    "    interests_social=2, \n",
    "    interests_investigative=5,\n",
    "    skills_analytical=5, \n",
    "    skills_communication=3, \n",
    "    skills_technical=4, \n",
    "    skills_problem_solving=5,\n",
    "    academic_science=5, \n",
    "    academic_humanities=2, \n",
    "    academic_commerce=3\n",
    ")\n",
    "\n",
    "print(f\"Top predicted career: {prediction}\\n\")\n",
    "print(\"Top 5 career recommendations:\")\n",
    "for career, prob in top_careers:\n",
    "    print(f\"{career}: {prob:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions for our example input\n",
    "plt.figure(figsize=(10, 6))\n",
    "careers, scores = zip(*top_careers)\n",
    "colors = ['#FF9999' if i == 0 else '#99CCFF' for i in range(len(top_careers))]\n",
    "plt.bar(careers, scores, color=colors)\n",
    "plt.title('Top 5 Career Recommendations')\n",
    "plt.xlabel('Career')\n",
    "plt.ylabel('Confidence Score (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 100)\n",
    "for i, (_, score) in enumerate(zip(careers, scores)):\n",
    "    plt.text(i, score + 1, f\"{score:.1f}%\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download the Trained Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model and scaler for use in the Flask API\n",
    "files.download('career_knn_model.pkl')\n",
    "files.download('career_scaler.pkl')\n",
    "files.download('student_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Creating/loading and preprocessing student data\n",
    "2. **Exploratory Data Analysis**: Visualizing distributions and relationships\n",
    "3. **Model Training**: Finding the optimal K value and training a KNN model\n",
    "4. **Model Evaluation**: Calculating accuracy, confusion matrix, and other metrics\n",
    "5. **Feature Importance**: Identifying which features most influence career recommendations\n",
    "6. **Making Predictions**: Demonstrating how to use the model for new students\n",
    "\n",
    "For the web application:\n",
    "1. Download the model (`career_knn_model.pkl`) and scaler (`career_scaler.pkl`)\n",
    "2. Place them in your Flask app directory as specified in the SETUP-GUIDE.md\n",
    "3. The Flask API will load these files to make predictions for new users\n",
    "\n",
    "The model can be improved over time by:\n",
    "1. Collecting more real student data\n",
    "2. Experimenting with different algorithms (Random Forest, SVM, etc.)\n",
    "3. Adding more nuanced features about student preferences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
